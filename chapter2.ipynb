{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanw/eodp-companion/blob/main/chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2.3 Jacobians\n",
        "\n",
        "To compute the Jacobian of a function using Python, we'll use the `sympy` library, which provides tools for symbolic mathematics. The Jacobian matrix represents the first-order partial derivatives of a vector-valued function. It's particularly useful for understanding the behavior of multivariate functions.\n",
        "\n",
        "Let's say we have a function $ f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2 $ defined by:\n",
        "$ f(w) = \\begin{bmatrix} f_1(w) \\\\ f_2(w) \\end{bmatrix} = \\begin{bmatrix} w_1^2 + w_2^2 \\\\ e^{w_1} + w_2 \\end{bmatrix} $\n",
        "where $ w = (w_1, w_2) $ is the input vector.\n",
        "\n",
        "The Jacobian matrix $ J $ of $ f $ at any point $ w $ is given by:\n",
        "$ J(f)(w) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial w_1} & \\frac{\\partial f_1}{\\partial w_2} \\\\ \\frac{\\partial f_2}{\\partial w_1} & \\frac{\\partial f_2}{\\partial w_2} \\end{bmatrix} $\n",
        "\n",
        "Here's how you can compute the Jacobian using `sympy`:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FqtBi-trvao-"
      },
      "outputs": [
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\left[\\begin{matrix}2 w_{1} & 2 w_{2}\\\\e^{w_{1}} & 1\\end{matrix}\\right]$"
            ],
            "text/plain": [
              "Matrix([\n",
              "[   2*w1, 2*w2],\n",
              "[exp(w1),    1]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#2.2.3\n",
        "import sympy as sp\n",
        "\n",
        "# Define symbols\n",
        "w1, w2 = sp.symbols('w1 w2')\n",
        "\n",
        "# Define the function f(w) = [f1(w), f2(w)]\n",
        "f1 = w1**2 + w2**2\n",
        "f2 = sp.exp(w1) + w2\n",
        "\n",
        "# Construct the vector function\n",
        "f = sp.Matrix([f1, f2])\n",
        "\n",
        "# Define the input vector\n",
        "w = sp.Matrix([w1, w2])\n",
        "\n",
        "# Compute the Jacobian\n",
        "J = f.jacobian(w)\n",
        "\n",
        "# Display the Jacobian\n",
        "J\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This code snippet defines the symbols $ w_1 $ and $ w_2 $, constructs the vector-valued function $ f $, and then computes the Jacobian matrix of $ f $ with respect to the input vector $ w $. The result, `J`, is the Jacobian matrix that consists of the partial derivatives of $ f_1 $ and $ f_2 $ with respect to $ w_1 $ and $ w_2 $. Let's run this example and see the output.\n",
        "\n",
        "The Jacobian matrix of the function $ f $ with respect to the input vector $ w = (w_1, w_2) $ is:\n",
        "$ J(f)(w) = \\begin{bmatrix} 2w_1 & 2w_2 \\\\ e^{w_1} & 1 \\end{bmatrix} $\n",
        "\n",
        "This matrix represents the first-order partial derivatives of the components of $ f $ with respect to each of the input variables $ w_1 $ and $ w_2 $, providing a linear approximation of the function's behavior around any point $ w $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's consider a function $ f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3 $ with three input variables, defined as:\n",
        "$ f(w) = \\begin{bmatrix} f_1(w) \\\\ f_2(w) \\\\ f_3(w) \\end{bmatrix} = \\begin{bmatrix} w_1^2 + w_2^2 + w_3^2 \\\\ e^{w_1} + \\sin(w_2) + w_3 \\\\ w_1 \\cdot w_2 + \\ln(w_3 + 1) \\end{bmatrix} $\n",
        "where $ w = (w_1, w_2, w_3) $ is the input vector.\n",
        "\n",
        "The Jacobian matrix $ J $ of $ f $ at any point $ w $ is given by the matrix of all first-order partial derivatives of each component of $ f $ with respect to each of the input variables $ w_1, w_2, $ and $ w_3 $:\n",
        "$ J(f)(w) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial w_1} & \\frac{\\partial f_1}{\\partial w_2} & \\frac{\\partial f_1}{\\partial w_3} \\\\ \\frac{\\partial f_2}{\\partial w_1} & \\frac{\\partial f_2}{\\partial w_2} & \\frac{\\partial f_2}{\\partial w_3} \\\\ \\frac{\\partial f_3}{\\partial w_1} & \\frac{\\partial f_3}{\\partial w_2} & \\frac{\\partial f_3}{\\partial w_3} \\end{bmatrix} $\n",
        "\n",
        "Let's compute this Jacobian matrix using `sympy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define symbols\n",
        "w1, w2, w3 = sp.symbols('w1 w2 w3')\n",
        "\n",
        "# Define the function f(w) = [f1(w), f2(w), f3(w)]\n",
        "f1 = w1**2 + w2**2 + w3**2\n",
        "f2 = sp.exp(w1) + sp.sin(w2) + w3\n",
        "f3 = w1*w2 + sp.log(w3 + 1)\n",
        "\n",
        "# Construct the vector function\n",
        "f = sp.Matrix([f1, f2, f3])\n",
        "\n",
        "# Define the input vector\n",
        "w = sp.Matrix([w1, w2, w3])\n",
        "\n",
        "# Compute the Jacobian\n",
        "J = f.jacobian(w)\n",
        "\n",
        "# Display the Jacobian\n",
        "J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code snippet will compute the Jacobian matrix of the given function $ f $ with respect to the inputs $ w_1, w_2, $ and $ w_3 $. Let's execute it to see the matrix.\n",
        "\n",
        "The Jacobian matrix of the function $ f $ with three input variables $ w = (w_1, w_2, w_3) $ is:\n",
        "$ J(f)(w) = \\begin{bmatrix} 2w_1 & 2w_2 & 2w_3 \\\\ e^{w_1} & \\cos(w_2) & 1 \\\\ w_2 & w_1 & \\frac{1}{w_3 + 1} \\end{bmatrix} $\n",
        "\n",
        "This matrix provides the first-order partial derivatives of each component of $ f $ with respect to $ w_1, w_2, $ and $ w_3 $, offering insights into how the function $ f $ changes directionally in the 3-dimensional input space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Definition 2.7 (Differntiability, multi-output case)\n",
        "To illustrate the concept of Fréchet differentiability for a multi-output function using Python, let's work with a specific example function and demonstrate how to verify its differentiability at a given point according to the definition you provided.\n",
        "\n",
        "Consider the function $ f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2 $ defined by:\n",
        "$ f(w) = \\left[ \\begin{array}{c} f_1(w) \\\\ f_2(w) \\end{array} \\right] = \\left[ \\begin{array}{c} w_1^2 + w_2^2 \\\\ e^{w_1} + w_2^3 \\end{array} \\right] $\n",
        "where $ w = (w_1, w_2) $.\n",
        "\n",
        "The task is to verify that $ f $ is Fréchet differentiable at a particular point $ w $ (let's choose $ w = (1,1) $ for this example). According to the definition, we need to check that the limit of\n",
        "$ \\lim_{\\|v\\|_2 \\to 0} \\frac{\\|f(w + v) - f(w) - J(f)(w) \\cdot v\\|_2}{\\|v\\|_2} = 0 $\n",
        "where $ J(f)(w) $ is the Jacobian matrix of $ f $ at $ w $, and $ v $ is an arbitrary direction vector.\n",
        "\n",
        "We will compute this limit numerically for small values of $ \\|v\\|_2 $ to illustrate the concept of differentiability:\n",
        "\n",
        "1. Compute the Jacobian matrix $ J(f)(w) $ at $ w = (1,1) $.\n",
        "2. For a small vector $ v $, compute $ f(w + v) - f(w) - J(f)(w) \\cdot v $.\n",
        "3. Evaluate the limit by decreasing the magnitude of $ v $ and checking if the ratio approaches 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1e-08, 3.36182359982022e-08),\n",
              " (1e-07, 2.8305653658042586e-07),\n",
              " (1e-06, 7.73154756903754e-07),\n",
              " (1e-05, 1.630950054524941e-05),\n",
              " (0.0001, 4.9290469680653423e-05),\n",
              " (0.001, 0.0020854235492624366),\n",
              " (0.01, 0.023653324274140474),\n",
              " (0.1, 0.15198962364614899),\n",
              " (1.0, 2.754513204551928)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function f and its Jacobian at w = (1, 1)\n",
        "def f(w):\n",
        "    return np.array([w[0]**2 + w[1]**2, np.exp(w[0]) + w[1]**3])\n",
        "\n",
        "def J(w):\n",
        "    return np.array([[2*w[0], 2*w[1]], [np.exp(w[0]), 3*w[1]**2]])\n",
        "\n",
        "w = np.array([1, 1])  # The point of interest\n",
        "\n",
        "# Function to check Fréchet differentiability\n",
        "def check_differentiability(w, epsilons):\n",
        "    results = []\n",
        "    for epsilon in epsilons:\n",
        "        # Generate a small vector v with a magnitude of epsilon\n",
        "        v = np.random.rand(2) * epsilon\n",
        "        v_norm = np.linalg.norm(v, 2)\n",
        "        \n",
        "        # Compute f(w + v) - f(w) - J(f)(w) * v\n",
        "        diff = f(w + v) - f(w) - J(w) @ v\n",
        "        \n",
        "        # Compute the ratio\n",
        "        ratio = np.linalg.norm(diff, 2) / v_norm\n",
        "        results.append((epsilon, ratio))\n",
        "    return results\n",
        "\n",
        "# Check differentiability for a range of epsilon values\n",
        "epsilons = np.logspace(-8, 0, 9)\n",
        "results = check_differentiability(w, epsilons)\n",
        "\n",
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Let's implement the steps in Python to demonstrate this:\n",
        "\n",
        "The results of the numerical check for Fréchet differentiability at $ w = (1,1) $ for our function $ f $ are given by the ratios of $\\|f(w + v) - f(w) - J(f)(w) \\cdot v\\|_2$ over $\\|v\\|_2$ for decreasing magnitudes of $ v $ (from $1e-08$ up to $1$). The ratios are as follows:\n",
        "\n",
        "- For $ \\epsilon = 1e-08 $, the ratio is approximately $ 3.23e-08 $.\n",
        "- For $ \\epsilon = 1e-07 $, the ratio is approximately $ 1.06e-07 $.\n",
        "- For $ \\epsilon = 1e-06 $, the ratio is approximately $ 2.68e-06 $.\n",
        "- For $ \\epsilon = 1e-05 $, the ratio is approximately $ 5.75e-06 $.\n",
        "- For $ \\epsilon = 0.0001 $, the ratio is approximately $ 1.40e-04 $.\n",
        "- For $ \\epsilon = 0.001 $, the ratio is approximately $ 6.86e-04 $.\n",
        "- For $ \\epsilon = 0.01 $, the ratio is approximately $ 2.79e-02 $.\n",
        "- For $ \\epsilon = 0.1 $, the ratio is approximately $ 9.81e-02 $.\n",
        "- For $ \\epsilon = 1.0 $, the ratio is approximately $ 2.18 $.\n",
        "\n",
        "As the magnitude of $ v $ decreases (i.e., as we approach $ 0 $), the ratio $\\frac{\\|f(w + v) - f(w) - J(f)(w) \\cdot v\\|_2}{\\|v\\|_2}$ tends to $ 0 $, illustrating the concept of Fréchet differentiability. Specifically, for very small values of $ \\epsilon $, the ratio is very close to $ 0 $, consistent with the definition of Fréchet differentiability, which requires this limit to be $ 0 $. This numerical example supports that the given function $ f $ is Fréchet differentiable at the point $ w = (1,1) $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Definition 2.8 (Jacobian)\n",
        "To illustrate the concept of the Jacobian and how it can be used to compute the directional derivative of a function $ f: \\mathbb{R}^P \\rightarrow \\mathbb{R}^M $, let's consider a specific example function. We'll then use Python to compute its Jacobian and apply it to calculate the directional derivative along a given input direction $ v $.\n",
        "\n",
        "Let's define the function $ f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^2 $ by:\n",
        "$ f(w) = \\left[ \\begin{array}{c} f_1(w) = w_1^2 + w_2^2 + w_3 \\\\ f_2(w) = e^{w_1} + w_2 \\cdot w_3 \\end{array} \\right] $\n",
        "where $ w = (w_1, w_2, w_3) $ is the input vector in $\\mathbb{R}^3$.\n",
        "\n",
        "The Jacobian of $ f $ at any point $ w $ is given by the matrix of all partial derivatives of $ f_1 $ and $ f_2 $ with respect to $ w_1, w_2, $ and $ w_3 $:\n",
        "$ \\partial f(w) = \\left[ \\begin{array}{ccc} \\frac{\\partial f_1}{\\partial w_1} & \\frac{\\partial f_1}{\\partial w_2} & \\frac{\\partial f_1}{\\partial w_3} \\\\ \\frac{\\partial f_2}{\\partial w_1} & \\frac{\\partial f_2}{\\partial w_2} & \\frac{\\partial f_2}{\\partial w_3} \\end{array} \\right] $\n",
        "\n",
        "We'll then compute the directional derivative of $ f $ at $ w $ along an input direction $ v = (v_1, v_2, v_3) $ using the formula:\n",
        "$ \\partial f(w)[v] = \\partial f(w) \\cdot v $\n",
        "\n",
        "Here's the Python code to compute the Jacobian and the directional derivative:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function f\n",
        "def f(w):\n",
        "    return np.array([w[0]**2 + w[1]**2 + w[2], np.exp(w[0]) + w[1]*w[2]])\n",
        "\n",
        "# Define the Jacobian of f\n",
        "def J(w):\n",
        "    return np.array([\n",
        "        [2*w[0], 2*w[1], 1],\n",
        "        [np.exp(w[0]), w[2], w[1]]\n",
        "    ])\n",
        "\n",
        "# Compute the directional derivative of f at w along v\n",
        "def directional_derivative(w, v):\n",
        "    jacobian = J(w)\n",
        "    return np.dot(jacobian, v)\n",
        "\n",
        "# Example input\n",
        "w = np.array([1, 2, 3])  # Point of interest\n",
        "v = np.array([0.5, -1, 2])  # Direction vector\n",
        "\n",
        "# Compute the directional derivative\n",
        "dd = directional_derivative(w, v)\n",
        "\n",
        "dd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This code computes the Jacobian matrix of $ f $ at the point $ w = (1, 2, 3) $ and then uses it to calculate the directional derivative of $ f $ along the vector $ v = (0.5, -1, 2) $. Let's execute it and see the result.\n",
        "\n",
        "The directional derivative of the function $ f $ at the point $ w = (1, 2, 3) $ along the direction vector $ v = (0.5, -1, 2) $ is approximately $[-1, 2.35914091]$. This result is a vector in $\\mathbb{R}^2$ that indicates how the function $ f $ changes at the point $ w $ in the direction of $ v $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variaitions along inputs\n",
        "Variation along an input direction is used in several contexts in mathematics, physics, engineering, and especially in machine learning and optimization. It serves to understand how a function's output changes as its input is varied slightly in a specific direction. This concept is crucial for:\n",
        "\n",
        "1. **Gradient Descent and Optimization:**\n",
        "   - In optimization, particularly in the training of machine learning models like neural networks, the variation along input directions (given by the gradient of the loss function with respect to the model's parameters) guides the process of adjusting these parameters to minimize the loss function. This is the essence of gradient descent.\n",
        "   \n",
        "2. **Sensitivity Analysis:**\n",
        "   - In engineering and economics, understanding how changes in input variables affect the output can be crucial for decision-making and design. Variation along an input direction can quantify the sensitivity of the system or model to changes in each input variable.\n",
        "\n",
        "3. **Directional Derivatives and Gradient:**\n",
        "   - In mathematics, the concept of directional derivatives extends the idea of partial derivatives by considering the rate of change of a function as one moves from a point in a specific direction. This is often used to find the direction in which a function increases or decreases most rapidly at a point, represented by the gradient.\n",
        "\n",
        "4. **Physics and Dynamics:**\n",
        "   - In physics, understanding how a system responds to changes in initial conditions or external forces involves studying variations along these \"input\" directions. For example, in mechanics, the variation of a particle's position in response to forces (inputs) determines its trajectory.\n",
        "\n",
        "5. **Control Systems:**\n",
        "   - In control theory, the response of a system to changes or perturbations in control inputs is fundamental. Variation along input directions can help design controllers that make the system behave in a desired manner, ensuring stability and performance.\n",
        "\n",
        "6. **Economic Models:**\n",
        "   - In economics, models often study how changes in inputs (like capital, labor) affect outputs (like production, utility). Variation along input directions can help understand the elasticity of output with respect to various inputs, informing policy and investment decisions.\n",
        "\n",
        "7. **Computational Geometry and Graphics:**\n",
        "   - In computer graphics and computational geometry, variations along input directions can help in rendering, simulation, and modeling tasks by understanding how changing parameters (light direction, viewpoint, shape deformation) affects the final image or model.\n",
        "\n",
        "8. **Machine Learning Model Interpretability:**\n",
        "   - Variation along input directions is also used in interpreting machine learning models, such as through techniques like saliency maps, which indicate how changes in features affect the model's predictions. This is crucial for understanding model behavior, especially in high-stakes applications like healthcare and finance.\n",
        "\n",
        "In summary, variation along input directions is a fundamental concept that is widely applicable across disciplines, offering insights into the behavior and performance of systems, functions, and models in response to changes in their inputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variations along outputs\n",
        "Variation along output directions is used in several key contexts where the focus is on understanding and exploiting the way changes in the output space of a function or system can inform about its characteristics or guide certain processes. This concept is particularly important in areas such as optimization, sensitivity analysis, and multi-objective decision making. Here are some scenarios where variation along output directions is pivotal:\n",
        "\n",
        "1. **Multi-Objective Optimization:**\n",
        "   - In problems where there are multiple objectives to be optimized simultaneously, variation along output directions can help in understanding the trade-offs between different objectives. This is crucial in Pareto optimization where the goal is to find solutions that offer the best trade-offs among competing objectives.\n",
        "\n",
        "2. **Model Sensitivity and Robustness Analysis:**\n",
        "   - Analyzing how sensitive a model’s output is to variations in its input can be approached by examining variations in the output space. This is essential for assessing the robustness of models to perturbations or uncertainties in inputs, especially in critical applications such as financial forecasting, climate modeling, and safety-critical engineering systems.\n",
        "\n",
        "3. **Deep Learning and Feature Attribution:**\n",
        "   - In deep learning, particularly in interpreting neural network decisions, understanding how variations in the output (such as the probability assigned to different classes) relate to changes in input features is crucial. Techniques that explore output variations, like Integrated Gradients or Layer-wise Relevance Propagation, help in attributing the prediction to input features, enhancing model interpretability.\n",
        "\n",
        "4. **Economics and Utility Theory:**\n",
        "   - In economics, understanding how variations in outputs (such as utility or profit) due to changes in consumption or production levels inform about consumer behavior or firm strategies. This can guide policy decisions, market analysis, and economic modeling.\n",
        "\n",
        "5. **Control Theory and System Identification:**\n",
        "   - In control theory, exploring how the output of a system varies in response to different control signals or disturbances can provide insights into the system's dynamics and help in designing controllers that achieve desired output behaviors.\n",
        "\n",
        "6. **Physics and Engineering Simulations:**\n",
        "   - In simulations involving complex physical phenomena, studying how variations in output quantities (like stress, temperature, or velocity fields) in response to changing conditions or parameters can yield insights into the behavior of physical systems under various scenarios.\n",
        "\n",
        "7. **Environmental Science and Impact Assessment:**\n",
        "   - Assessing the impact of environmental changes or policies often involves understanding how these changes affect various output indicators, such as pollution levels, biodiversity, or ecosystem services. Variation along these output dimensions can inform conservation strategies and policy formulations.\n",
        "\n",
        "8. **Quantitative Finance and Risk Management:**\n",
        "   - In finance, analyzing how the outputs of financial models (such as the valuation of derivatives or the risk of a portfolio) vary with changes in underlying factors can provide critical insights for risk management, investment strategies, and regulatory compliance.\n",
        "\n",
        "Variation along output directions enables a nuanced understanding of system or model behavior in multi-dimensional output spaces, facilitating decision-making, interpretation, and analysis across a broad spectrum of disciplines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chain Rule\n",
        "To demonstrate the chain rule for functions with Python, let's consider two specific functions $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$ and $g: \\mathbb{R}^3 \\rightarrow \\mathbb{R}$, where $f$ is differentiable at a point $w \\in \\mathbb{R}^2$ and $g$ is differentiable at $f(w) \\in \\mathbb{R}^3$. We'll then compute the Jacobian of the composition $g \\circ f$ at $w$, illustrating the chain rule.\n",
        "\n",
        "Let's define:\n",
        "1. $f(w) = \\left[ \\begin{array}{c} w_1^2 \\\\ w_1w_2 \\\\ w_2^2 \\end{array} \\right]$, where $w = (w_1, w_2)$.\n",
        "2. $g(x) = x_1 + x_2^2 + e^{x_3}$, where $x = (x_1, x_2, x_3)$ is the output of $f(w)$.\n",
        "\n",
        "According to the proposition, the Jacobian of $g \\circ f$ at $w$, $\\partial(g \\circ f)(w)$, is given by the product of $\\partial g(f(w))$ and $\\partial f(w)$.\n",
        "\n",
        "First, we'll define $f$ and $g$, compute their Jacobians, and then calculate the Jacobian of the composition $g \\circ f$ at a specific point $w$, say $w = (1, 1)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4.        , 7.43656366])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function f\n",
        "def f(w):\n",
        "    return np.array([w[0]**2, w[0]*w[1], w[1]**2])\n",
        "\n",
        "# Define the Jacobian of f\n",
        "def J_f(w):\n",
        "    return np.array([\n",
        "        [2*w[0], 0],\n",
        "        [w[1], w[0]],\n",
        "        [0, 2*w[1]]\n",
        "    ])\n",
        "\n",
        "# Define the function g\n",
        "def g(x):\n",
        "    return x[0] + x[1]**2 + np.exp(x[2])\n",
        "\n",
        "# Define the gradient (Jacobian for scalar output function) of g\n",
        "def grad_g(x):\n",
        "    return np.array([1, 2*x[1], np.exp(x[2])])\n",
        "\n",
        "# Compute the Jacobian of the composition g ∘ f at w\n",
        "def J_g_f(w):\n",
        "    x = f(w)\n",
        "    return np.dot(grad_g(x), J_f(w))\n",
        "\n",
        "# Example input\n",
        "w = np.array([1, 1])  # Point of interest\n",
        "\n",
        "# Compute the Jacobian of the composition g ∘ f\n",
        "jacobian_g_f = J_g_f(w)\n",
        "\n",
        "jacobian_g_f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This code calculates the Jacobian of $g \\circ f$ at the point $w = (1, 1)$ by first finding the output of $f$ at $w$, then computing the gradient of $g$ at this output, and finally applying the chain rule as described. Let's execute this code to see the result.\n",
        "\n",
        "The Jacobian of the composition $g \\circ f$ at the point $w = (1, 1)$ is approximately $[4.00, 7.44]$. This result represents the gradient of the composed function $g(f(w))$ with respect to the input $w$, effectively applying the chain rule to combine the derivatives of $f$ and $g$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definition 2.9 (Inner Product)\n",
        "\n",
        "To demonstrate the concept of an inner product and the induced norm and distance in Python, let's define a simple vector space $E = \\mathbb{R}^n$ where the inner product is the standard dot product. The dot product satisfies the properties of bilinearity, symmetry, and positive definiteness as required for an inner product. We'll show how to compute the inner product, the induced norm, and the distance between two vectors in this space.\n",
        "\n",
        "### Inner Product\n",
        "The inner product between two vectors $w$ and $v$ in $E$ is given by:\n",
        "$ \\langle w, v \\rangle = w_1v_1 + w_2v_2 + \\ldots + w_nv_n = w \\cdot v $\n",
        "\n",
        "### Norm\n",
        "The norm of a vector $w$ induced by the inner product is:\n",
        "$ \\|w\\| = \\sqrt{\\langle w, w \\rangle} $\n",
        "\n",
        "### Distance\n",
        "The distance between two vectors $w$ and $v$ in $E$ is defined as the norm of their difference:\n",
        "$ \\|w - v\\| = \\sqrt{\\langle w - v, w - v \\rangle} $\n",
        "\n",
        "Let's implement these concepts in Python using `numpy`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define vectors\n",
        "w = np.array([1, 2, 3])\n",
        "v = np.array([4, 5, 6])\n",
        "\n",
        "# Inner product\n",
        "def inner_product(w, v):\n",
        "    return np.dot(w, v)\n",
        "\n",
        "# Norm induced by the inner product\n",
        "def norm(w):\n",
        "    return np.sqrt(inner_product(w, w))\n",
        "\n",
        "# Distance between two vectors\n",
        "def distance(w, v):\n",
        "    return norm(w - v)\n",
        "\n",
        "# Compute the inner product, norm, and distance\n",
        "inner_prod = inner_product(w, v)\n",
        "norm_w = norm(w)\n",
        "dist_w_v = distance(w, v)\n",
        "\n",
        "inner_prod, norm_w, dist_w_v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example calculates the inner product between two vectors $w$ and $v$, the norm of $w$, and the distance between $w$ and $v$ using the standard dot product as the inner product. Let's run this code to see the outputs.\n",
        "\n",
        "The inner product between vectors $w$ and $v$ is $32$, the norm of vector $w$ induced by the inner product is approximately $3.74$, and the distance between vectors $w$ and $v$ is approximately $5.20$. These calculations illustrate how to apply the concepts of inner product, norm, and distance in a vector space using Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Definition 2.10 (Adjoint operator)\n",
        "\n",
        "To illustrate the concept of the adjoint operator of a linear map using Python, let's consider two Euclidean spaces $E$ and $F$ represented by real-valued matrices or vectors, and a linear map represented by a matrix $A$. In this context, the inner product is the standard dot product, and the adjoint of a linear map $A : E \\rightarrow F$ (represented by matrix multiplication) can be understood as the transpose of the matrix $A$, denoted $A^\\top$, which maps $F \\rightarrow E$.\n",
        "\n",
        "Given $A$, a matrix representing our linear map, and vectors $v \\in E$ and $u \\in F$, the property defining the adjoint $A^*$ (here $A^\\top$) can be expressed as:\n",
        "$ \\langle Av, u \\rangle_F = \\langle v, A^\\top u \\rangle_E $\n",
        "\n",
        "This property holds because, for matrices and vectors, the inner product $\\langle \\cdot, \\cdot \\rangle$ is equivalent to the dot product, and the dot product of the matrix-vector product $Av$ with $u$ is equal to the dot product of $v$ with the matrix-vector product $A^\\top u$.\n",
        "\n",
        "Let's demonstrate this with an example in Python:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a matrix A representing the linear map l\n",
        "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Define vectors v in E and u in F\n",
        "v = np.array([1, 2])\n",
        "u = np.array([3, 4, 5])\n",
        "\n",
        "# Compute the inner products\n",
        "left_side = np.dot(A @ v, u)  # ⟨l[v], u⟩F\n",
        "right_side = np.dot(v, A.T @ u)  # ⟨v, l∗[u]⟩E\n",
        "\n",
        "left_side, right_side"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This code sets up a matrix $A$ representing our linear map $l$, and two vectors $v$ and $u$ in spaces $E$ and $F$, respectively. It then computes both sides of the equation defining the adjoint operator to verify their equality. Let's run the example and observe the results.\n",
        "\n",
        "Both sides of the equation defining the adjoint operator yield the same result, $144$, confirming that $\\langle Av, u \\rangle_F = \\langle v, A^\\top u \\rangle_E$. This demonstrates the concept of the adjoint of a linear map in the context of Euclidean spaces equipped with standard inner products, using Python for the computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Definition 2.12 (Jacobian-vector product)\n",
        "\n",
        "To illustrate the concept of the Jacobian-vector product (JVP) with Python, let's consider a simple differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ and demonstrate how to compute the JVP. The JVP represents the application of the Jacobian matrix of $f$ at a point $w$ to a vector $v$, effectively providing a way to compute directional derivatives.\n",
        "\n",
        "As an example, let's choose a function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ defined by $f(w) = [w_1^2 + w_2, w_1 + w_2^2]$, where $w = [w_1, w_2]$ is the input vector. We will compute the JVP of $f$ at a point $w$ in the direction of a vector $v$.\n",
        "\n",
        "First, we compute the Jacobian matrix of $f$ with respect to $w$, then apply this Jacobian to the vector $v$ to obtain the JVP.\n",
        "\n",
        "Let's choose $w = [1, 2]$ and $v = [0.5, -0.5]$ for our example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function f\n",
        "def f(w):\n",
        "    return np.array([w[0]**2 + w[1], w[0] + w[1]**2])\n",
        "\n",
        "# Define the Jacobian of f\n",
        "def jacobian_f(w):\n",
        "    return np.array([\n",
        "        [2*w[0], 1],\n",
        "        [1, 2*w[1]]\n",
        "    ])\n",
        "\n",
        "# Compute the Jacobian-vector product (JVP)\n",
        "def jvp(w, v):\n",
        "    J = jacobian_f(w)\n",
        "    return np.dot(J, v)\n",
        "\n",
        "# Example input\n",
        "w = np.array([1, 2])\n",
        "v = np.array([0.5, -0.5])\n",
        "\n",
        "# Compute the JVP\n",
        "jvp_result = jvp(w, v)\n",
        "\n",
        "jvp_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code computes the JVP for our function $f$ at the point $w$ in the direction of $v$. The `jvp` function calculates the Jacobian matrix of $f$ at $w$ and then applies it to $v$. Let's run this code to obtain the JVP.\n",
        "\n",
        "The Jacobian-vector product (JVP) of the function $f$ at the point $w = [1, 2]$ in the direction of $v = [0.5, -0.5]$ is $[0.5, -1.5]$. This result demonstrates how the Jacobian matrix of $f$ at a given point can be used to compute directional derivatives in a specified direction, providing insights into how the function changes along that direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To demonstrate the concept of the vector-Jacobian product (VJP) using Python, let's work with a differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ and show how to compute the VJP. The VJP, $\\partial f(w)^*[u]$, gives us the gradient of the function $f$ projected along an output direction $u \\in F$, which is particularly useful in backpropagation in neural networks for computing gradients of scalar-valued loss functions with respect to high-dimensional parameters.\n",
        "\n",
        "As an example, let's consider the same function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ defined by $f(w) = [w_1^2 + w_2, w_1 + w_2^2]$, where $w = [w_1, w_2]$ is the input vector. To compute the VJP, we need an output direction $u \\in \\mathbb{R}^m$, the Jacobian matrix of $f$ at $w$, and then we apply the adjoint of the Jacobian (which for real-valued functions corresponds to its transpose) to $u$.\n",
        "\n",
        "For our example, let's choose $w = [1, 2]$ and an output direction $u = [0.5, -0.5]$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function f\n",
        "def f(w):\n",
        "    return np.array([w[0]**2 + w[1], w[0] + w[1]**2])\n",
        "\n",
        "# Define the Jacobian of f\n",
        "def jacobian_f(w):\n",
        "    return np.array([\n",
        "        [2*w[0], 1],\n",
        "        [1, 2*w[1]]\n",
        "    ])\n",
        "\n",
        "# Compute the Vector-Jacobian Product (VJP)\n",
        "def vjp(w, u):\n",
        "    J = jacobian_f(w)\n",
        "    return np.dot(J.T, u)\n",
        "\n",
        "# Example input\n",
        "w = np.array([1, 2])\n",
        "u = np.array([0.5, -0.5])\n",
        "\n",
        "# Compute the VJP\n",
        "vjp_result = vjp(w, u)\n",
        "\n",
        "vjp_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This code snippet calculates the VJP for the function $f$ at the point $w$ in the output direction $u$, using the transpose of the Jacobian matrix of $f$ applied to $u$. Let's execute this code to see the VJP result.\n",
        "\n",
        "The Vector-Jacobian Product (VJP) for the function $f$ at the point $w = [1, 2]$ in the output direction $u = [0.5, -0.5]$ is $[0.5, -1.5]$. This demonstrates how to project the gradient of a function along a specified output direction, a key operation in gradient-based optimization and learning algorithms."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNIeRuQ5YSQRhon3RVYFbUu",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
